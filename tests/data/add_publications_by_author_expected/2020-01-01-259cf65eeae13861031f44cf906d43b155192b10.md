---
title: Explicitly Modeling Syntax in Language Model improves Generalization
venue: ArXiv
names: Yikang Shen, Shawn Tan, Alessandro Sordoni, Siva Reddy, Aaron C. Courville
tags:
- ArXiv
link: https://www.semanticscholar.org/paper/259cf65eeae13861031f44cf906d43b155192b10
author: Siva Reddy
categories: Publications

---

*{{ page.names }}*

**{{ page.venue }}**

{% include display-publication-links.html pub=page %}

## Abstract

Syntax is fundamental to our thinking about language. Although neural networks are very successful in many tasks, they do not explicitly model syntactic structure. Failing to capture the structure of inputs could lead to generalization problems and over-parametrization. In the present work, we propose a new syntax-aware language model: Syntactic Ordered Memory (SOM). The model explicitly models the structure with a one-step look-ahead parser and maintains the conditional probability setting of the standard language model. Experiments show that SOM can achieve strong results in language modeling and syntactic generalization tests, while using fewer parameters then other models.